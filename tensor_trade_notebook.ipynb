{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#    Tensor Trade with LSTM series prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Collection of thoughts and notes about my experience trying to beat the market. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Resources:\n",
    "GITHUB Author @NotAdamKing\n",
    "https://github.com/cauchyturing/UCR_Time_Series_Classification_Deep_Learning_Baseline \n",
    "https://discordapp.com/channels/592446624882491402/593538654857723909\n",
    "https://towardsdatascience.com/trade-smarter-w-reinforcement-learning-a5e91163f315 \n",
    "\n",
    "GITHUB Author @hootnuot\n",
    "https://github.com/hootnot/oandapyV20-examples \n",
    "http://developer.oanda.com/rest-live-v20/instrument-ep/\n",
    "\n",
    "GITHUB Author @philipperemy\n",
    "https://github.com/philipperemy/FX-1-Minute-Data/blob/master/download_all_fx_data.py \n",
    "\n",
    "https://en.wikipedia.org/wiki/Sortino_ratio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Get sources for Forex: \n",
    "    We can add more exchanges to the tickers list which is used throughout the modules.\n",
    "Note: Most of the tickers are in the {CUR_CUR} format. This snippet needs the \"EURUSD\" so we need to replace it. Data is stored in a local SQL DB with the bid and ask price. Currently, we use the ask and resample this in the OHLC format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://github.com/philipperemy/FX-1-Minute-Data/blob/master/download_all_fx_data.py GITHUB Author @philipperemy\n",
    "from api import download_fx_m1_data, download_fx_m1_data_year\n",
    " tickers = [\"EUR_USD\"]\n",
    "    for ticker in tickers:\n",
    "        exchange = ticker\n",
    "        ticker = ticker.replace(\"_\", \"\")\n",
    "        year = int(2018)\n",
    "        print(ticker)\n",
    "        output_folder = os.path.join('output', ticker)\n",
    "        mkdir_p(output_folder)\n",
    "        try:\n",
    "            while True:\n",
    "                could_download_full_year = False\n",
    "                try:\n",
    "                    output_filename = download_fx_m1_data_year(year, ticker)\n",
    "                    shutil.move(output_filename, os.path.join(output_folder, output_filename))\n",
    "                    could_download_full_year = True\n",
    "                except:\n",
    "                    pass  # lets download it month by month.\n",
    "                month = 1\n",
    "                while not could_download_full_year and month <= 12:\n",
    "                    output_filename = download_fx_m1_data(str(year), str(month), ticker)\n",
    "                    shutil.move(output_filename, os.path.join(output_folder, output_filename))\n",
    "                    month += 1\n",
    "                year += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Stock\tAsk\tDate\n",
    "EUR_USD\t1.10372\t10/11/2019 16:39\n",
    "EUR_USD\t1.10372\t10/11/2019 16:40\n",
    "EUR_USD\t1.10372\t10/11/2019 16:41\n",
    "EUR_USD\t1.10362\t10/11/2019 16:42\n",
    "EUR_USD\t1.10358\t10/11/2019 16:43\n",
    "EUR_USD\t1.10359\t10/11/2019 16:44\n",
    "EUR_USD\t1.10358\t10/11/2019 16:45\n",
    "EUR_USD\t1.10354\t10/11/2019 16:47\n",
    "EUR_USD\t1.10353\t10/11/2019 16:48\n",
    "EUR_USD\t1.10353\t10/11/2019 16:49\n",
    "EUR_USD\t1.10356\t10/11/2019 16:50\n",
    "EUR_USD\t1.10355\t10/11/2019 16:51\n",
    "EUR_USD\t1.10358\t10/11/2019 16:52\n",
    "EUR_USD\t1.10363\t10/11/2019 16:53\n",
    "EUR_USD\t1.10357\t10/11/2019 16:54\n",
    "EUR_USD\t1.10348\t10/11/2019 16:55\n",
    "EUR_USD\t1.10343\t10/11/2019 16:56\n",
    "EUR_USD\t1.10353\t10/11/2019 16:57\n",
    "EUR_USD\t1.10363\t10/11/2019 16:58"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random samples are chosen for training and reverse sorted. This is to ensure that we are training on real-world shuffled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_sql(\"select * from (select date as Date, ask as Open, ask as Low, ask as High, ask as Close from forex_daily_price where exchange = '{}' and date between '{}' and '{}'  ORDER BY date DESC LIMIT {}) tmp order by tmp.date ASC\".format(ticker, s_date, e_date, limit), conn)\n",
    "resampled = grouped_data[target].resample(period).ohlc()\n",
    "resampled['Date'] = pd.to_datetime(resampled['Date'], format=\"%Y-%m-%d %H:%M:%S\").astype('datetime64[ns]').view('int64')\n",
    "#target is the exchange we wish to train on.\n",
    "#period is the period we wish to train (ie 5T, 10T, 15T etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \"Load a random param for training. Currently, I don't find that one data set does better than the next. For each iteration, I load a random param and save the highest score. There are 15 total iterations with randomly chosen params.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "After each training episode is complete I run a test on another smaller sample set; save and assign an overall score. The lowest scores are deleted leaving only the top 5\n",
    "The accuracy is measured using with a test sample and predicting the next Nth in series. We look ahead and measure the percent difference from the market.\n",
    "\n",
    "Test results:\n",
    "    Mean error: 0.000061\n",
    "    Accuracy: 0.888671\n",
    "\n",
    "##For use with live data\n",
    "#http://developer.oanda.com/rest-live-v20/instrument-ep/\n",
    "#https://github.com/hootnot/oandapyV20-examples\n",
    "#gather the last 1000 points ie: (period * 1000)\n",
    "from oandapyV20 import API\n",
    "for i in self.clargs.instruments:\n",
    "                r = instruments.InstrumentsCandles(instrument=i, params=params)\n",
    "                rv = self.api.request(r)\n",
    "                kw = {}\n",
    "                if self.clargs.nice:\n",
    "                    kw = {\"indent\": self.clargs.nice}\n",
    "                print(\"{}\".format(json.dumps(rv, **kw)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "once training accuracy is above the threshold, it will automatically begin backtesting the SQL DB. Updating the results. The new table will look like: \n",
    "    Date|Open|High|Low|Close|5T_Open|5T_High|5T_Low|5T_Close|5T_Confidence|10T_Open|10T_High|10T_Low|10T_Close|10T_Confidence|15T_Open|15T_High|15T_Low|15T_Close|15T_Confidence  \n",
    "    I have noticed when the candles flatten out the accuracy of the predictions will drop tremendously. This works best when the data is volatile.\n",
    "    I can test the data by the following formula (actual * confidence) - prediction This should be very close to 0.\n",
    "    I may add this formula to an SQL query at a later date. For now I load a sample set into excel and check it with the described formula.\n",
    "    example: actual value was 1.00315 and predicted value was 1.002899~ or 0.0250% error  #sample for p-5T with Open\n",
    "    =((1.002899 - 1.00315) / 1.00315) / 1.00315\n",
    "\n",
    "I may add in more features if I see an increase with the newly added features. \n",
    "My control ran for ep: 1000 : st: 1000 with a reward of -0.0068\n",
    "#Current status of new features is null. \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensortrade.exchanges.simulated import SimulatedExchange\n",
    "from tensortrade.features.scalers import MinMaxNormalizer\n",
    "from tensortrade.features.stationarity import FractionalDifference\n",
    "from tensortrade.features import FeaturePipeline\n",
    "from tensortrade.rewards.risk_adjusted_return_strategy import RiskAdjustedReturnStrategy\n",
    "from tensortrade.actions import DiscreteActionStrategy\n",
    "from tensortrade.environments import TradingEnvironment\n",
    "from stable_baselines.common.policies import MlpLnLstmPolicy\n",
    "from stable_baselines import A2C\n",
    "from tensortrade.strategies import StableBaselinesTradingStrategy\n",
    "import pandas as pd\n",
    "from tensortrade.exchanges.simulated import SimulatedExchange\n",
    "from tensortrade.features import FeaturePipeline\n",
    "from tensortrade.features.scalers import MinMaxNormalizer\n",
    "from tensortrade.features.stationarity import FractionalDifference\n",
    "from tensortrade.features.indicators import SimpleMovingAverage\n",
    "import mysql.connector\n",
    "\n",
    "reward_strategy = RiskAdjustedReturnStrategy()\n",
    "action_strategy = DiscreteActionStrategy(n_actions=20,\n",
    "                                        instrument_symbol='ETH/BTC')\n",
    "\n",
    "#Thought: Adjust n_actions by the confidence score for that metric. Set a maximum for lower scores and a minimum for higher scores\n",
    "\n",
    "model = A2C\n",
    "policy = MlpLnLstmPolicy\n",
    "params = { \"learning_rate\": 1e-5 }\n",
    "performance = strategy.run(steps=10000, episodes=10000)\n",
    "#running multiple steps, multiple episodes as I have read other research papers claiming better results.\n",
    "strategy.save_agent(path=\"a2c_btc_1h\")\n",
    "normalize_price = MinMaxNormalizer([\"Date\",\"Open\",\"High\",\"Low\",\"Close\",\"5T_Open\",\"5T_High\",\"5T_Low\",\"5T_Close\",\"5T_Confidence\",\"10T_Open\",\"10T_High\",\"10T_Low\",\"10T_Close\",\"10T_Confidence\",\"15T_Open\",\"15T_High\",\"15T_Low\",\"15T_Close\",\"15T_Confidence\" ])\n",
    "#I dont think I need to normalize all my columns; I will try a sample to see how things go.\n",
    "strategy.save_agent(path=\"data/a2c_{}_{}_{}_{}\".format(ticker, period, steps, episodes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@isaksson delux\n",
    "trading fees depend on the platform you want to trade on, so they vary. you might include it in your reward function. same goes for funding rate if u want to simulate margin\n",
    " \n",
    "@Chuba\n",
    "They also vary by trade type\n",
    "Limits vs market order.\n",
    "\n",
    "Need to look into this closer as this can break the best trade stategy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RiskAdjustedReturnStrategy(RewardStrategy):\n",
    "    returns = self._exchange.performance['net_worth'].diff()\n",
    "    risk_adjusted_return = self._return_algorithm(returns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "When I trade manually I wait until the price crosses over the support or resistance line. This indicates the price is about to break out an go up or down. This may make a good parameter to add to our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm not the creator of this snippet but cannot site the original owner.\n",
    "def get_upper_lower():\n",
    "\tdf = pd.read_sql(\"select * from (select date, ask from forex_daily_price where exchange = '{}' ORDER BY date DESC LIMIT {}) tmp order by tmp.date ASC\".format(ticker, limit), conn)\n",
    "\t# group by day and drop NA values (usually weekends)\n",
    "\tticks_data = grouped_data['Sell'].resample('1min').ohlc()\n",
    "\t# use 'ask'\n",
    "\tsell_data = grouped_data.as_matrix(columns=['Sell'])\n",
    "\t# calculate bandwidth (expirement with quantile and samples)\n",
    "\tbandwidth = estimate_bandwidth(sell_data, quantile=0.1, n_samples=100)\n",
    "\tms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "\t\n",
    "\t# fit the data\n",
    "\tms.fit(sell_data)\n",
    "\t\n",
    "\tml_results = []\n",
    "\tfor k in range(len(np.unique(ms.labels_))):\n",
    "\t\tmy_members = ms.labels_ == k\n",
    "\t\tvalues = sell_data[my_members, 0]\n",
    "\t\t\n",
    "\t\t# find the edges\n",
    "\t\tlower = ml_results.append(min(values))\n",
    "\t\tupper = ml_results.append(max(values))\n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
